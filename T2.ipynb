{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPWH7XNO8nZM"
   },
   "source": [
    "# HW 2: Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fncLvGe28nZN"
   },
   "source": [
    "In this homework you will be building several varieties of language models.\n",
    "\n",
    "## Goal\n",
    "\n",
    "We ask that you construct the following models in Torch / NamedTensor:\n",
    "\n",
    "1. A count-based trigram model with linear-interpolation. $$p(y_t | y_{1:t-1}) =  \\alpha_1 p(y_t | y_{t-2}, y_{t-1}) + \\alpha_2 p(y_t | y_{t-1}) + (1 - \\alpha_1 - \\alpha_2) p(y_t) $$\n",
    "2. A neural network language model (consult *A Neural Probabilistic Language Model* http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "3. An LSTM language model (consult *Recurrent Neural Network Regularization*, https://arxiv.org/pdf/1409.2329.pdf) \n",
    "4. Your own extensions to these models.\n",
    "\n",
    "\n",
    "Consult the papers provided for hyperparameters.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxPRHeF08nZO"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6dq9Ut782YG"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchtext opt_einsum\n",
    "!pip install -qU git+https://github.com/harvardnlp/namedtensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nqdDeot8nZP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "from namedtensor import ntorch\n",
    "from namedtensor.text import NamedField\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GlHfcng8nZS"
   },
   "source": [
    "The dataset we will use of this problem is known as the Penn Treebank (http://aclweb.org/anthology/J93-2004). It is the most famous dataset in NLP and includes a large set of different types of annotations. We will be using it here in a simple case as just a language modeling dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQFcSB478nZU"
   },
   "source": [
    "To start, `torchtext` requires that we define a mapping from the raw text data to featurized indices. These fields make it easy to map back and forth between readable data and math, which helps for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QE3gF0D8nZU"
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = NamedField(names=(\"seqlen\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tk5DN87J8nZW"
   },
   "source": [
    "Next we input our data. Here we will use the first 10k sentences of the standard PTB language modeling split, and tell it the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "nICj0CXD-C2Z",
    "outputId": "ac6bc67e-d505-4ced-9627-1408af02dec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  185k  100  185k    0     0   922k      0 --:--:-- --:--:-- --:--:--  926k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  595k  100  595k    0     0  2173k      0 --:--:-- --:--:-- --:--:-- 2165k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4982k  100 4982k    0     0  6038k      0 --:--:-- --:--:-- --:--:-- 6031k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  390k  100  390k    0     0  1895k      0 --:--:-- --:--:-- --:--:-- 1895k\n"
     ]
    }
   ],
   "source": [
    "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/input.txt\n",
    "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/train.5k.txt\n",
    "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/train.txt\n",
    "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/valid.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rza-uvgD8nZX"
   },
   "outputs": [],
   "source": [
    "# Data distributed with the assignment\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQYymdMu8nZa"
   },
   "source": [
    "The data format for language modeling is strange. We pretend the entire corpus is one long sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsGKL1jS8nZb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 1\n"
     ]
    }
   ],
   "source": [
    "print('len(train)', len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qzWC4vz18nZg"
   },
   "source": [
    "Here's the vocab itself. (This dataset has unk symbols already, but torchtext adds its own.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7d9evM2z8nZh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 10001\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISe-Tx3g8nZk"
   },
   "source": [
    "When debugging you may want to use a smaller vocab size. This will run much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkwV12tZ8nZl"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    TEXT.build_vocab(train, max_size=1000)\n",
    "    len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WoYwv9rx8nZn"
   },
   "source": [
    "The batching is done in a strange way for language modeling. Each element of the batch consists of `bptt_len` words in order. This makes it easy to run recurrent models like RNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwqEsvmMGnHT"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.iterator import BPTTIterator\n",
    "from torchtext.data import Batch, Dataset\n",
    "import math\n",
    " \n",
    "\n",
    "class NamedBpttIterator(BPTTIterator):\n",
    "    def __iter__(self):\n",
    "        text = self.dataset[0].text\n",
    "        TEXT = self.dataset.fields['text']\n",
    "        TEXT.eos_token = None\n",
    "        text = text + ([TEXT.pad_token] * int(math.ceil(len(text) / self.batch_size)\n",
    "                                              * self.batch_size - len(text)))\n",
    "        data = TEXT.numericalize(\n",
    "            [text], device=self.device)\n",
    "        data = (data\n",
    "            .stack((\"seqlen\", \"batch\"), \"flat\")\n",
    "            .split(\"flat\", (\"batch\", \"seqlen\"), batch=self.batch_size)\n",
    "            .transpose(\"seqlen\", \"batch\")\n",
    "        )\n",
    "\n",
    "        dataset = Dataset(examples=self.dataset.examples, fields=[\n",
    "            ('text', TEXT), ('target', TEXT)])\n",
    "        while True:\n",
    "            for i in range(0, len(self) * self.bptt_len, self.bptt_len):\n",
    "                self.iterations += 1\n",
    "                seq_len = min(self.bptt_len, len(data) - i - 1)\n",
    "                yield Batch.fromvars(\n",
    "                    dataset, self.batch_size,\n",
    "                    text = data.narrow(\"seqlen\", i, seq_len),\n",
    "                    target = data.narrow(\"seqlen\", i+1, seq_len),\n",
    "                )\n",
    "                         \n",
    "            if not self.repeat:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fkF0hMZU8nZo"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = NamedBpttIterator.splits(\n",
    "    (train, val, test), batch_size=10, bptt_len=32, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZpKaUXPo8nZq"
   },
   "source": [
    "Here's what these batches look like. Each is a string of length 32. Sentences are ended with a special `<eos>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTCxJ-Mz8nZr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text batch [max bptt length, batch size] OrderedDict([('seqlen', 32), ('batch', 10)])\n",
      "Second in batch NamedTensor(\n",
      "\ttensor([  72,    5,   28,  247,   61,   12,  216,    5,    0, 1847,   10,    4,\n",
      "          72,  547,    3, 6506,  163,    7,  105,  479,   38,   31,  295, 4901,\n",
      "          13,    4,   49,    3,    0,    0,   25, 2471]),\n",
      "\t('seqlen',))\n",
      "Converted back to string:  shares of its common stock for each of <unk> deposit 's N shares outstanding <eos> liberty national a bank holding company has assets exceeding $ N billion <eos> <unk> <unk> was appointed\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it) \n",
    "print(\"Size of text batch [max bptt length, batch size]\", batch.text.shape)\n",
    "example = batch.text[{\"batch\": 1}]\n",
    "print(\"Second in batch\", example)\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in example.values.data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXNZ-aXy8nZu"
   },
   "source": [
    "The next batch will be the continuation of the previous. This is helpful for running recurrent neural networks where you remember the current state when transitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYqdPsk-8nZv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted back to string:  president and chief executive officer of this financially troubled department store chain effective nov. N succeeding frank robertson who is retiring early <eos> mr. <unk> was previously president and chief operating officer\n"
     ]
    }
   ],
   "source": [
    "batch = next(it)\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[{\"batch\": 1}].values.data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phnGMV-G8nZy"
   },
   "source": [
    "The batch object also contains the targets for the given batch in the field `target`. The target is simply the text offset by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiABGRXoSmea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted back to string:  and chief executive officer of this financially troubled department store chain effective nov. N succeeding frank robertson who is retiring early <eos> mr. <unk> was previously president and chief operating officer of\n"
     ]
    }
   ],
   "source": [
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.target.get(\"batch\", 1).values.data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvMmJgTN8nZz"
   },
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "Using the data given by this iterator, you should construct 3 different torch models that take in batch.text and produce a distribution over the next word. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/harvard-cs287-s19-hw2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QzcCGOM8nZz"
   },
   "source": [
    "For the final Kaggle test, we will have you do a next word prediction task. We will provide a 10 word prefix of sentences, and it is your job to predict 20 possible next word candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "enEfiDQ78nZ0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but while the new york stock exchange did n't fall ___\n",
      "some circuit breakers installed after the october N crash failed ___\n",
      "the N stock specialist firms on the big board floor ___\n",
      "big investment banks refused to step up to the plate ___\n",
      "heavy selling of standard & poor 's 500-stock index futures ___\n",
      "seven big board stocks ual amr bankamerica walt disney capital ___\n",
      "once again the specialists were not able to handle the ___\n",
      "<unk> james <unk> chairman of specialists henderson brothers inc. it ___\n",
      "when the dollar is in a <unk> even central banks ___\n",
      "speculators are calling for a degree of liquidity that is ___\n"
     ]
    }
   ],
   "source": [
    "!head input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvQXwMrh8nZ3"
   },
   "source": [
    "As a sample Kaggle submission, let us build a simple unigram model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esywMIzm8nZ5"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "count = Counter()\n",
    "for b in iter(train_iter):\n",
    "    count.update(b.text.values.contiguous().view(-1).tolist())\n",
    "count[TEXT.vocab.stoi[\"<eos>\"]] = 0\n",
    "predictions = [TEXT.vocab.itos[i] for i, c in count.most_common(20)]\n",
    "with open(\"sample.txt\", \"w\") as fout:\n",
    "    print(\"id,word\", file=fout)\n",
    "    for i, l in enumerate(open(\"input.txt\"), 1):\n",
    "        print(\"%d,%s\"%(i, \" \".join(predictions)), file=fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldI2WGre8naC"
   },
   "source": [
    "The metric we are using is mean average precision of your 20-best list. \n",
    "\n",
    "$$MAP@20 = \\frac{1}{|D|} \\sum_{u=1}^{|D|} \\sum_{k=1}^{20} Precision(u, 1:k)$$\n",
    "\n",
    "Ideally we would use log-likelihood or ppl as discussed in class, but this is the best Kaggle gives us. This takes into account whether you got the right answer and how highly you ranked it. \n",
    "\n",
    "In particular, we ask that you do not game this metric. Please submit *exactly 20* unique predictions for each example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GdflwooW8naD"
   },
   "source": [
    "As always you should put up a 5-6 page write-up following the template provided in the repository: https://github.com/harvard-ml-courses/nlp-template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models/lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(TEXT, embedding_dim=100, hidden_dim=100)\n",
    "optim = torch.optim.Adam(lstm.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('batch', 10), ('seqlen', 32), ('vocab', 10001)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm(batch.text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('seqlen', 32), ('batch', 10)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " _, best_words = ntorch.topk(lstm(batch.text)[{'seqlen': -1}],\n",
    "                                            'vocab', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_target = batch.target[{'seqlen': -1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedTensor(\n",
       "\ttensor([2171,    5,    8,    2,  128,   18,  300,    6,    6,  303]),\n",
       "\t('batch',))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_loss(model, batch):\n",
    "     _, best_words = ntorch.topk(model(batch.text)[{'seqlen': -1}], 'vocab', 20)\n",
    "    last_target = batch.target[{'seqlen': -1}]\n",
    "    is_correct = best_words == last_target\n",
    "    scores = torch.cumsum(is_correct.values, 1).float() / (1 + torch.arange(20)).float()\n",
    "    return scores.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_words[{'vocab': 0}] = last_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = best_words == last_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.cumsum(is_correct.values, 1).float() / (1 + torch.arange(20)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(35.9774)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "        19, 20])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(20)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('batch', 10), ('vocab', 20)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a9c817676c4275ba48ca59c1126c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2905), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-02b4901d93fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         progress_bar=True)\n\u001b[0m",
      "\u001b[0;32m~/Documents/Francisco/Harvard/08 Senior Spring/CS 287/psets/2/models/utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_fn, optimizer, train_iter, val_iter, num_epochs, writer, callback, inner_callback, progress_bar)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def cb(**kwargs):\n",
    "    ep, t, v = (kwargs['epoch'], (kwargs['train_loss'] / len(train_iter)), (kwargs['val_loss'] / len(val_iter)))\n",
    "    print(f'Epoch {ep}, train loss = {t:.2f} [{np.exp(t):.2f}], val loss = {v:.2f} [{np.exp(v):.2f}]')\n",
    "\n",
    "train_model(lstm, lstm_loss, optim, \n",
    "        train_iter, val_iter, num_epochs=15, \n",
    "        callback=cb,\n",
    "        progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azureml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-652c1ae46e07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azureml'"
     ]
    }
   ],
   "source": [
    "import azureml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azureml-sdk\n",
      "  Using cached https://files.pythonhosted.org/packages/ff/08/042fc0fc6126054869fc087998184ea7c99ff6db8a0aee0bcb7d9a82a090/azureml_sdk-1.0.15.1-py3-none-any.whl\n",
      "Collecting azureml-train==1.0.15.* (from azureml-sdk)\n",
      "  Using cached https://files.pythonhosted.org/packages/cc/eb/dce049020d70c4322bb6fc4b1cfd2ec7915ba88802509a26d0751b3b76c5/azureml_train-1.0.15-py3-none-any.whl\n",
      "Collecting azureml-pipeline==1.0.15.* (from azureml-sdk)\n",
      "  Using cached https://files.pythonhosted.org/packages/a6/8e/3765eab01a13db1b32b1f6e4a8ec6b897ea329eee5c7028cfe983008ab6e/azureml_pipeline-1.0.15-py3-none-any.whl\n",
      "Collecting azureml-core==1.0.15.* (from azureml-sdk)\n",
      "  Using cached https://files.pythonhosted.org/packages/a9/a6/dd85766160ec29c31200c15b88756fe4fceb8681396e72b2afa32cda712e/azureml_core-1.0.15-py2.py3-none-any.whl\n",
      "Collecting azureml-train-core==1.0.15.* (from azureml-train==1.0.15.*->azureml-sdk)\n",
      "  Using cached https://files.pythonhosted.org/packages/4a/77/21a8b6a1b7bb682deb6060f68e04863b6cb0a0883097fed4f37cdc04c48d/azureml_train_core-1.0.15-py3-none-any.whl\n",
      "Collecting azureml-pipeline-steps==1.0.15.* (from azureml-pipeline==1.0.15.*->azureml-sdk)\n",
      "  Using cached https://files.pythonhosted.org/packages/4d/b0/6e9b05d46859d942e41bd1718a29c55e00f89aa41075c173c81c1ec76d78/azureml_pipeline_steps-1.0.15-py3-none-any.whl\n",
      "Collecting azureml-pipeline-core==1.0.15.* (from azureml-pipeline==1.0.15.*->azureml-sdk)\n",
      "  Using cached https://files.pythonhosted.org/packages/4c/dd/0ad1b416ec5aea019d1f460bb209aaf31dbd2cec191feeac7e7f78106050/azureml_pipeline_core-1.0.15-py3-none-any.whl\n",
      "Requirement already satisfied: docker in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (3.7.0)\n",
      "Requirement already satisfied: PyJWT in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (1.7.1)\n",
      "Collecting azure-mgmt-containerregistry>=2.0.0 (from azureml-core==1.0.15.*->azureml-sdk)\n",
      "  Using cached https://files.pythonhosted.org/packages/7a/4b/06040d992f93531e32c5f7cf7884f3edfec11f76f802dd9224c1116c3129/azure_mgmt_containerregistry-2.7.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: azure-mgmt-resource>=1.2.1 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (2.1.0)\n",
      "Requirement already satisfied: azure-common>=1.1.12 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (1.1.18)\n",
      "Requirement already satisfied: ruamel.yaml<=0.15.51,>=0.15.35 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (0.15.51)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (2.7.5)\n",
      "Requirement already satisfied: azure-storage-common>=1.1.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (1.4.0)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.23 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (1.23)\n",
      "Requirement already satisfied: backports.tempfile in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (1.0)\n",
      "Requirement already satisfied: azure-graphrbac>=0.40.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (0.53.0)\n",
      "Requirement already satisfied: adal>=1.2.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (1.2.1)\n",
      "Requirement already satisfied: azure-mgmt-storage>=1.5.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (3.1.1)\n",
      "Requirement already satisfied: msrest>=0.5.1 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (0.6.4)\n",
      "Requirement already satisfied: msrestazure>=0.4.33 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (0.6.0)\n",
      "Requirement already satisfied: pathspec in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (0.5.0)\n",
      "Requirement already satisfied: requests>=2.19.1 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (2.21.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (1.12.0)\n",
      "Requirement already satisfied: contextlib2 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (0.5.5)\n",
      "Requirement already satisfied: azure-storage-nspkg>=3.0.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (3.1.0)\n",
      "Requirement already satisfied: ndg-httpsclient in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (0.5.1)\n",
      "Requirement already satisfied: paramiko>=2.0.8 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (2.4.2)\n",
      "Requirement already satisfied: azure-mgmt-keyvault>=0.40.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (1.1.0)\n",
      "Requirement already satisfied: azure-mgmt-authorization>=0.40.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (0.51.1)\n",
      "Requirement already satisfied: jmespath in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (0.9.3)\n",
      "Requirement already satisfied: azure-storage-blob>=1.1.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (1.5.0)\n",
      "Requirement already satisfied: SecretStorage<3.0.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (2.3.1)\n",
      "Requirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.* in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (2.3.1)\n",
      "Requirement already satisfied: pytz in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (2018.9)\n",
      "Collecting pyyaml>=3.13 (from azureml-core==1.0.15.*->azureml-sdk)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (0.4.1)\n",
      "Requirement already satisfied: pyopenssl>=17.1.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (19.0.0)\n",
      "Requirement already satisfied: jsonpickle in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-core==1.0.15.*->azureml-sdk) (1.1)\n",
      "Collecting azureml-telemetry==1.0.15.* (from azureml-train-core==1.0.15.*->azureml-train==1.0.15.*->azureml-sdk)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/33/07e162febdac4c102da95cfe5b01ab2664c411fb1950a5a6333cce506724/azureml_telemetry-1.0.15-py3-none-any.whl\n",
      "Collecting azureml-train-restclients-hyperdrive==1.0.15.* (from azureml-train-core==1.0.15.*->azureml-train==1.0.15.*->azureml-sdk)\n",
      "  Using cached https://files.pythonhosted.org/packages/10/7f/86b9b6d9aab19f63626ffbc7fed51187fd078c8aa95357b30252861acfa3/azureml_train_restclients_hyperdrive-1.0.15-py3-none-any.whl\n",
      "Requirement already satisfied: certifi in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-pipeline-steps==1.0.15.*->azureml-pipeline==1.0.15.*->azureml-sdk) (2018.1.18)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from docker->azureml-core==1.0.15.*->azureml-sdk) (0.44.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from docker->azureml-core==1.0.15.*->azureml-sdk) (0.4.0)\n",
      "Requirement already satisfied: backports.weakref in /anaconda/envs/py36/lib/python3.6/site-packages (from backports.tempfile->azureml-core==1.0.15.*->azureml-sdk) (1.0.post1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from msrest>=0.5.1->azureml-core==1.0.15.*->azureml-sdk) (1.2.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from msrest>=0.5.1->azureml-core==1.0.15.*->azureml-sdk) (0.6.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda/envs/py36/lib/python3.6/site-packages (from requests>=2.19.1->azureml-core==1.0.15.*->azureml-sdk) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda/envs/py36/lib/python3.6/site-packages (from requests>=2.19.1->azureml-core==1.0.15.*->azureml-sdk) (3.0.4)\n",
      "Requirement already satisfied: azure-nspkg>=2.0.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azure-storage-nspkg>=3.0.0->azureml-core==1.0.15.*->azureml-sdk) (3.0.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.1 in /anaconda/envs/py36/lib/python3.6/site-packages (from ndg-httpsclient->azureml-core==1.0.15.*->azureml-sdk) (0.4.2)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /anaconda/envs/py36/lib/python3.6/site-packages (from paramiko>=2.0.8->azureml-core==1.0.15.*->azureml-sdk) (3.1.6)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /anaconda/envs/py36/lib/python3.6/site-packages (from paramiko>=2.0.8->azureml-core==1.0.15.*->azureml-sdk) (1.3.0)\n",
      "Requirement already satisfied: azure-mgmt-nspkg>=2.0.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from azure-mgmt-keyvault>=0.40.0->azureml-core==1.0.15.*->azureml-sdk) (3.0.2)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core==1.0.15.*->azureml-sdk) (0.22.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.7 in /anaconda/envs/py36/lib/python3.6/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core==1.0.15.*->azureml-sdk) (1.10.0)\n",
      "Requirement already satisfied: applicationinsights in /anaconda/envs/py36/lib/python3.6/site-packages (from azureml-telemetry==1.0.15.*->azureml-train-core==1.0.15.*->azureml-train==1.0.15.*->azureml-sdk) (0.11.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/py36/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.1->azureml-core==1.0.15.*->azureml-sdk) (3.0.1)\n",
      "Requirement already satisfied: pycparser in /anaconda/envs/py36/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.7->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core==1.0.15.*->azureml-sdk) (2.17)\n",
      "\u001b[31mawsebcli 3.12.1 has requirement colorama==0.3.7, but you'll have colorama 0.4.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mawsebcli 3.12.1 has requirement requests<=2.9.1,>=2.6.1, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: azure-mgmt-containerregistry, pyyaml, azureml-core, azureml-telemetry, azureml-train-restclients-hyperdrive, azureml-train-core, azureml-train, azureml-pipeline-core, azureml-pipeline-steps, azureml-pipeline, azureml-sdk\n",
      "  Found existing installation: PyYAML 3.12\n",
      "\u001b[31mCannot uninstall 'PyYAML'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install azureml-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azureml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-48d3f61b4a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azureml'"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of CS 287 T2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
