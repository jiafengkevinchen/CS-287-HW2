{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CS 287 T2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "zPWH7XNO8nZM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# HW 2: Language Modeling"
      ]
    },
    {
      "metadata": {
        "id": "fncLvGe28nZN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this homework you will be building several varieties of language models.\n",
        "\n",
        "## Goal\n",
        "\n",
        "We ask that you construct the following models in Torch / NamedTensor:\n",
        "\n",
        "1. A count-based trigram model with linear-interpolation. $$p(y_t | y_{1:t-1}) =  \\alpha_1 p(y_t | y_{t-2}, y_{t-1}) + \\alpha_2 p(y_t | y_{t-1}) + (1 - \\alpha_1 - \\alpha_2) p(y_t) $$\n",
        "2. A neural network language model (consult *A Neural Probabilistic Language Model* http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
        "3. An LSTM language model (consult *Recurrent Neural Network Regularization*, https://arxiv.org/pdf/1409.2329.pdf) \n",
        "4. Your own extensions to these models.\n",
        "\n",
        "\n",
        "Consult the papers provided for hyperparameters.\n",
        "\n",
        " \n"
      ]
    },
    {
      "metadata": {
        "id": "TxPRHeF08nZO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system."
      ]
    },
    {
      "metadata": {
        "id": "s6dq9Ut782YG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchtext opt_einsum\n",
        "!pip install -qU git+https://github.com/harvardnlp/namedtensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4nqdDeot8nZP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "from namedtensor import ntorch\n",
        "from namedtensor.text import NamedField\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-GlHfcng8nZS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset we will use of this problem is known as the Penn Treebank (http://aclweb.org/anthology/J93-2004). It is the most famous dataset in NLP and includes a large set of different types of annotations. We will be using it here in a simple case as just a language modeling dataset."
      ]
    },
    {
      "metadata": {
        "id": "UQFcSB478nZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To start, `torchtext` requires that we define a mapping from the raw text data to featurized indices. These fields make it easy to map back and forth between readable data and math, which helps for debugging."
      ]
    },
    {
      "metadata": {
        "id": "7QE3gF0D8nZU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our input $x$\n",
        "TEXT = NamedField(names=(\"seqlen\",))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tk5DN87J8nZW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we input our data. Here we will use the first 10k sentences of the standard PTB language modeling split, and tell it the fields."
      ]
    },
    {
      "metadata": {
        "id": "g0mCaCjMZzsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "65774bcf-70a1-4629-bb59-bcd86ad76399"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jiafengkevinchen/CS-287-HW2.git\n",
        "!mv CS-287-HW2/* .\n",
        "!rm -r CS-287-HW2/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CS-287-HW2'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/136)   \u001b[K\rremote: Counting objects:   1% (2/136)   \u001b[K\rremote: Counting objects:   2% (3/136)   \u001b[K\rremote: Counting objects:   3% (5/136)   \u001b[K\rremote: Counting objects:   4% (6/136)   \u001b[K\rremote: Counting objects:   5% (7/136)   \u001b[K\rremote: Counting objects:   6% (9/136)   \u001b[K\rremote: Counting objects:   7% (10/136)   \u001b[K\rremote: Counting objects:   8% (11/136)   \u001b[K\rremote: Counting objects:   9% (13/136)   \u001b[K\rremote: Counting objects:  10% (14/136)   \u001b[K\rremote: Counting objects:  11% (15/136)   \u001b[K\rremote: Counting objects:  12% (17/136)   \u001b[K\rremote: Counting objects:  13% (18/136)   \u001b[K\rremote: Counting objects:  14% (20/136)   \u001b[K\rremote: Counting objects:  15% (21/136)   \u001b[K\rremote: Counting objects:  16% (22/136)   \u001b[K\rremote: Counting objects:  17% (24/136)   \u001b[K\rremote: Counting objects:  18% (25/136)   \u001b[K\rremote: Counting objects:  19% (26/136)   \u001b[K\rremote: Counting objects:  20% (28/136)   \u001b[K\rremote: Counting objects:  21% (29/136)   \u001b[K\rremote: Counting objects:  22% (30/136)   \u001b[K\rremote: Counting objects:  23% (32/136)   \u001b[K\rremote: Counting objects:  24% (33/136)   \u001b[K\rremote: Counting objects:  25% (34/136)   \u001b[K\rremote: Counting objects:  26% (36/136)   \u001b[K\rremote: Counting objects:  27% (37/136)   \u001b[K\rremote: Counting objects:  28% (39/136)   \u001b[K\rremote: Counting objects:  29% (40/136)   \u001b[K\rremote: Counting objects:  30% (41/136)   \u001b[K\rremote: Counting objects:  31% (43/136)   \u001b[K\rremote: Counting objects:  32% (44/136)   \u001b[K\rremote: Counting objects:  33% (45/136)   \u001b[K\rremote: Counting objects:  34% (47/136)   \u001b[K\rremote: Counting objects:  35% (48/136)   \u001b[K\rremote: Counting objects:  36% (49/136)   \u001b[K\rremote: Counting objects:  37% (51/136)   \u001b[K\rremote: Counting objects:  38% (52/136)   \u001b[K\rremote: Counting objects:  39% (54/136)   \u001b[K\rremote: Counting objects:  40% (55/136)   \u001b[K\rremote: Counting objects:  41% (56/136)   \u001b[K\rremote: Counting objects:  42% (58/136)   \u001b[K\rremote: Counting objects:  43% (59/136)   \u001b[K\rremote: Counting objects:  44% (60/136)   \u001b[K\rremote: Counting objects:  45% (62/136)   \u001b[K\rremote: Counting objects:  46% (63/136)   \u001b[K\rremote: Counting objects:  47% (64/136)   \u001b[K\rremote: Counting objects:  48% (66/136)   \u001b[K\rremote: Counting objects:  49% (67/136)   \u001b[K\rremote: Counting objects:  50% (68/136)   \u001b[K\rremote: Counting objects:  51% (70/136)   \u001b[K\rremote: Counting objects:  52% (71/136)   \u001b[K\rremote: Counting objects:  53% (73/136)   \u001b[K\rremote: Counting objects:  54% (74/136)   \u001b[K\rremote: Counting objects:  55% (75/136)   \u001b[K\rremote: Counting objects:  56% (77/136)   \u001b[K\rremote: Counting objects:  57% (78/136)   \u001b[K\rremote: Counting objects:  58% (79/136)   \u001b[K\rremote: Counting objects:  59% (81/136)   \u001b[K\rremote: Counting objects:  60% (82/136)   \u001b[K\rremote: Counting objects:  61% (83/136)   \u001b[K\rremote: Counting objects:  62% (85/136)   \u001b[K\rremote: Counting objects:  63% (86/136)   \u001b[K\rremote: Counting objects:  64% (88/136)   \u001b[K\rremote: Counting objects:  65% (89/136)   \u001b[K\rremote: Counting objects:  66% (90/136)   \u001b[K\rremote: Counting objects:  67% (92/136)   \u001b[K\rremote: Counting objects:  68% (93/136)   \u001b[K\rremote: Counting objects:  69% (94/136)   \u001b[K\rremote: Counting objects:  70% (96/136)   \u001b[K\rremote: Counting objects:  71% (97/136)   \u001b[K\rremote: Counting objects:  72% (98/136)   \u001b[K\rremote: Counting objects:  73% (100/136)   \u001b[K\rremote: Counting objects:  74% (101/136)   \u001b[K\rremote: Counting objects:  75% (102/136)   \u001b[K\rremote: Counting objects:  76% (104/136)   \u001b[K\rremote: Counting objects:  77% (105/136)   \u001b[K\rremote: Counting objects:  78% (107/136)   \u001b[K\rremote: Counting objects:  79% (108/136)   \u001b[K\rremote: Counting objects:  80% (109/136)   \u001b[K\rremote: Counting objects:  81% (111/136)   \u001b[K\rremote: Counting objects:  82% (112/136)   \u001b[K\rremote: Counting objects:  83% (113/136)   \u001b[K\rremote: Counting objects:  84% (115/136)   \u001b[K\rremote: Counting objects:  85% (116/136)   \u001b[K\rremote: Counting objects:  86% (117/136)   \u001b[K\rremote: Counting objects:  87% (119/136)   \u001b[K\rremote: Counting objects:  88% (120/136)   \u001b[K\rremote: Counting objects:  89% (122/136)   \u001b[K\rremote: Counting objects:  90% (123/136)   \u001b[K\rremote: Counting objects:  91% (124/136)   \u001b[K\rremote: Counting objects:  92% (126/136)   \u001b[K\rremote: Counting objects:  93% (127/136)   \u001b[K\rremote: Counting objects:  94% (128/136)   \u001b[K\rremote: Counting objects:  95% (130/136)   \u001b[K\rremote: Counting objects:  96% (131/136)   \u001b[K\rremote: Counting objects:  97% (132/136)   \u001b[K\rremote: Counting objects:  98% (134/136)   \u001b[K\rremote: Counting objects:  99% (135/136)   \u001b[K\rremote: Counting objects: 100% (136/136)   \u001b[K\rremote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 136 (delta 77), reused 126 (delta 67), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (136/136), 3.82 MiB | 16.63 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nICj0CXD-C2Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ac6bc67e-d505-4ced-9627-1408af02dec1"
      },
      "cell_type": "code",
      "source": [
        "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/input.txt\n",
        "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/train.5k.txt\n",
        "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/train.txt\n",
        "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/valid.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  185k  100  185k    0     0  1655k      0 --:--:-- --:--:-- --:--:-- 1655k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  595k  100  595k    0     0  5462k      0 --:--:-- --:--:-- --:--:-- 5462k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4982k  100 4982k    0     0  32.4M      0 --:--:-- --:--:-- --:--:-- 32.4M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  390k  100  390k    0     0  3614k      0 --:--:-- --:--:-- --:--:-- 3614k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rza-uvgD8nZX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data distributed with the assignment\n",
        "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
        "    path=\".\", \n",
        "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PQYymdMu8nZa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data format for language modeling is strange. We pretend the entire corpus is one long sentence."
      ]
    },
    {
      "metadata": {
        "id": "hsGKL1jS8nZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('len(train)', len(train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qzWC4vz18nZg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's the vocab itself. (This dataset has unk symbols already, but torchtext adds its own.)"
      ]
    },
    {
      "metadata": {
        "id": "7d9evM2z8nZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train)\n",
        "print('len(TEXT.vocab)', len(TEXT.vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ISe-Tx3g8nZk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When debugging you may want to use a smaller vocab size. This will run much faster."
      ]
    },
    {
      "metadata": {
        "id": "ZkwV12tZ8nZl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    TEXT.build_vocab(train, max_size=1000)\n",
        "    len(TEXT.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoYwv9rx8nZn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The batching is done in a strange way for language modeling. Each element of the batch consists of `bptt_len` words in order. This makes it easy to run recurrent models like RNNs. "
      ]
    },
    {
      "metadata": {
        "id": "BwqEsvmMGnHT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data.iterator import BPTTIterator\n",
        "from torchtext.data import Batch, Dataset\n",
        "import math\n",
        " \n",
        "\n",
        "class NamedBpttIterator(BPTTIterator):\n",
        "    def __iter__(self):\n",
        "        text = self.dataset[0].text\n",
        "        TEXT = self.dataset.fields['text']\n",
        "        TEXT.eos_token = None\n",
        "        text = text + ([TEXT.pad_token] * int(math.ceil(len(text) / self.batch_size)\n",
        "                                              * self.batch_size - len(text)))\n",
        "        data = TEXT.numericalize(\n",
        "            [text], device=self.device)\n",
        "        data = (data\n",
        "            .stack((\"seqlen\", \"batch\"), \"flat\")\n",
        "            .split(\"flat\", (\"batch\", \"seqlen\"), batch=self.batch_size)\n",
        "            .transpose(\"seqlen\", \"batch\")\n",
        "        )\n",
        "\n",
        "        dataset = Dataset(examples=self.dataset.examples, fields=[\n",
        "            ('text', TEXT), ('target', TEXT)])\n",
        "        while True:\n",
        "            for i in range(0, len(self) * self.bptt_len, self.bptt_len):\n",
        "                self.iterations += 1\n",
        "                seq_len = min(self.bptt_len, len(data) - i - 1)\n",
        "                yield Batch.fromvars(\n",
        "                    dataset, self.batch_size,\n",
        "                    text = data.narrow(\"seqlen\", i, seq_len),\n",
        "                    target = data.narrow(\"seqlen\", i+1, seq_len),\n",
        "                )\n",
        "                         \n",
        "            if not self.repeat:\n",
        "                return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkF0hMZU8nZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_iter, val_iter, test_iter = NamedBpttIterator.splits(\n",
        "    (train, val, test), batch_size=10, device=torch.device(\"cuda\"), bptt_len=32, repeat=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZpKaUXPo8nZq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's what these batches look like. Each is a string of length 32. Sentences are ended with a special `<eos>` token."
      ]
    },
    {
      "metadata": {
        "id": "ZTCxJ-Mz8nZr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "it = iter(train_iter)\n",
        "batch = next(it) \n",
        "print(\"Size of text batch [max bptt length, batch size]\", batch.text.shape)\n",
        "example = batch.text[{\"batch\": 1}]\n",
        "print(\"Second in batch\", example)\n",
        "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in example.values.data]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vXNZ-aXy8nZu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next batch will be the continuation of the previous. This is helpful for running recurrent neural networks where you remember the current state when transitioning."
      ]
    },
    {
      "metadata": {
        "id": "wYqdPsk-8nZv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = next(it)\n",
        "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[{\"batch\": 1}].values.data]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "phnGMV-G8nZy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The batch object also contains the targets for the given batch in the field `target`. The target is simply the text offset by one."
      ]
    },
    {
      "metadata": {
        "id": "KiABGRXoSmea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.target.get(\"batch\", 1).values.data]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DvMmJgTN8nZz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assignment\n",
        "\n",
        "Now it is your turn to build the models described at the top of the assignment. \n",
        "\n",
        "Using the data given by this iterator, you should construct 3 different torch models that take in batch.text and produce a distribution over the next word. \n",
        "\n",
        "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/harvard-cs287-s19-hw2/"
      ]
    },
    {
      "metadata": {
        "id": "0QzcCGOM8nZz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the final Kaggle test, we will have you do a next word prediction task. We will provide a 10 word prefix of sentences, and it is your job to predict 20 possible next word candidates"
      ]
    },
    {
      "metadata": {
        "id": "enEfiDQ78nZ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head input.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EvQXwMrh8nZ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As a sample Kaggle submission, let us build a simple unigram model.  "
      ]
    },
    {
      "metadata": {
        "id": "esywMIzm8nZ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "count = Counter()\n",
        "for b in iter(train_iter):\n",
        "    count.update(b.text.values.contiguous().view(-1).tolist())\n",
        "count[TEXT.vocab.stoi[\"<eos>\"]] = 0\n",
        "predictions = [TEXT.vocab.itos[i] for i, c in count.most_common(20)]\n",
        "with open(\"sample.txt\", \"w\") as fout:\n",
        "    print(\"id,word\", file=fout)\n",
        "    for i, l in enumerate(open(\"input.txt\"), 1):\n",
        "        print(\"%d,%s\"%(i, \" \".join(predictions)), file=fout)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ldI2WGre8naC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The metric we are using is mean average precision of your 20-best list. \n",
        "\n",
        "$$MAP@20 = \\frac{1}{|D|} \\sum_{u=1}^{|D|} \\sum_{k=1}^{20} Precision(u, 1:k)$$\n",
        "\n",
        "Ideally we would use log-likelihood or ppl as discussed in class, but this is the best Kaggle gives us. This takes into account whether you got the right answer and how highly you ranked it. \n",
        "\n",
        "In particular, we ask that you do not game this metric. Please submit *exactly 20* unique predictions for each example.\n"
      ]
    },
    {
      "metadata": {
        "id": "GdflwooW8naD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As always you should put up a 5-6 page write-up following the template provided in the repository: https://github.com/harvard-ml-courses/nlp-template"
      ]
    }
  ]
}