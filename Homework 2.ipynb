{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will be building several varieties of language models.\n",
    "\n",
    "## Goal\n",
    "\n",
    "We ask that you construct the following models in PyTorch:\n",
    "\n",
    "1. A trigram model with linear-interpolation. $$p(y_t | y_{1:t-1}) =  \\alpha_1 p(y_t | y_{t-2}, y_{t-1}) + \\alpha_2 p(y_t | y_{t-1}) + (1 - \\alpha_1 - \\alpha_2) p(y_t) $$\n",
    "2. A neural network language model (consult *A Neural Probabilistic Language Model* http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "3. An LSTM language model (consult *Recurrent Neural Network Regularization*, https://arxiv.org/pdf/1409.2329.pdf) \n",
    "4. Your own extensions to these models...\n",
    "\n",
    "\n",
    "Consult the papers provided for hyperparameters.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchtext opt_einsum\n",
    "!pip install -qU git+https://github.com/harvardnlp/namedtensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models/trigram.py\n",
    "%run models/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch.sparse as sp\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch import nn\n",
    "from namedtensor import ntorch\n",
    "import namedtensor.nn as nnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will use of this problem is known as the Penn Treebank (http://aclweb.org/anthology/J93-2004). It is the most famous dataset in NLP and includes a large set of different types of annotations. We will be using it here in a simple case as just a language modeling dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, `torchtext` requires that we define a mapping from the raw text data to featurized indices. These fields make it easy to map back and forth between readable data and math, which helps for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we input our data. Here we will use the first 10k sentences of the standard PTB language modeling split, and tell it the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distributed with the assignment\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data format for language modeling is strange. We pretend the entire corpus is one long sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 1\n"
     ]
    }
   ],
   "source": [
    "print('len(train)', len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the vocab itself. (This dataset has unk symbols already, but torchtext adds its own.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 10001\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debugging you may want to use a smaller vocab size. This will run much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    TEXT.build_vocab(train, max_size=1000)\n",
    "    len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batching is done in a strange way for language modeling. Each element of the batch consists of `bptt_len` words in order. This makes it easy to run recurrent models like RNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, bptt_len=32, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what these batches look like. Each is a string of length 32. Sentences are ended with a special `<eos>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text batch [max bptt length, batch size] torch.Size([32, 10])\n",
      "Second in batch tensor([   8,  202,   77,    5,  183,  561, 3837,   18,  975,  976,    7,  943,\n",
      "           5,  157,   78, 1571,  289,  645,    3,   30,  132,    0,   20,    2,\n",
      "         273, 7821,   17,    9,  117, 2815,  969,    6])\n",
      "Converted back to string:  in part because of buy programs generated by stock-index arbitrage a form of program trading involving futures contracts <eos> but interest <unk> as the day wore on and investors looked ahead to\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it) \n",
    "print(\"Size of text batch [max bptt length, batch size]\", batch.text.size())\n",
    "print(\"Second in batch\", batch.text[:, 2])\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 2].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next batch will be the continuation of the previous. This is helpful for running recurrent neural networks where you remember the current state when transitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted back to string:  the release later this week of two important economic reports <eos> the first is wednesday 's survey of purchasing managers considered a good indicator of how the nation 's manufacturing sector fared\n"
     ]
    }
   ],
   "source": [
    "batch = next(it)\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 2].data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say they also find it <unk> that cbs news is apparently concentrating on mr. hoffman 's problems as a <unk> <eos> this is dangerous and <unk> abbie 's life says ms. <unk>\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    print(tensor_to_text(batch.text[:,3], TEXT))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9972,     5,   202,    39,     9,    99,  1176,   654,   374,     4],\n",
       "        [ 9973,    28,    77,    60,   630,    90,    20,   271,    39,    49],\n",
       "        [ 9975,   247,     5,   678,   564,  2255,     7,     9,   276,  1077],\n",
       "        [ 9976,    61,   183,    15,     9,     2,     0,   501,     0,    13],\n",
       "        [ 9977,    12,   561,     0,   224,   313,   155,   274,  8018,     4],\n",
       "        [ 9981,   216,  3837,    11,   185,  1642,    24,  1560,     3,    22],\n",
       "        [ 9982,     5,    18,  1017,   128,     5,  1891,     3,  2786,    70],\n",
       "        [ 9983,     0,   975,   310,    19,  1064,    31,    15,  3660,    41],\n",
       "        [ 9984,  1847,   976,    14,     7,     9,     7,  3020,  4360,     3],\n",
       "        [ 9985,    10,     7,  1078,   829,   714,     0,     6,    81,  1324],\n",
       "        [ 9987,     4,   943,  6361,     0,     2,     0,   586,   635,   160],\n",
       "        [ 9988,    72,     5,    17,     3,   265,     3,    85,    28,    18],\n",
       "        [ 9989,   547,   157,    24,     2,   821,   199,  3138,   335,     2],\n",
       "        [ 9990,     3,    78,  4491,   325,     3,   104,     3,   182,  5438],\n",
       "        [ 9992,  6506,  1571,    10,   159,  9782,   171,    15,  7859,   188],\n",
       "        [ 9993,   163,   289,   356,    12,  2003,    24,   696,   308,  2390],\n",
       "        [ 9994,     7,   645,    20,     7,   547,  2635,  1609,  1571,   168],\n",
       "        [ 9995,   105,     3,     7,   301,    35,  6237,  2342,     7,     5],\n",
       "        [ 9996,   479,    30,     0,   630,  2952,     6,     6,   338,  1685],\n",
       "        [ 9997,    38,   132,     3,   225,    18,    50,     2,     0,  1974],\n",
       "        [ 9998,    31,     0,    40,    11,    13,  2258,    38,   155,  3464],\n",
       "        [ 9999,   295,    20,    14,   271,     4,    24,    23,     3,    51],\n",
       "        [10000,  4901,     2,  3582,    44,    49,  1891,    74,     7,    44],\n",
       "        [    3,    13,   273,     9,    13,   156,     0,    11,    36,     4],\n",
       "        [ 9257,     4,  7821,     0,     4,     4,     6,   864,    93,     4],\n",
       "        [    0,    49,    17,  7238,     8,   121,     7,    12,    61,     5],\n",
       "        [    4,     3,     9,    10,     2,  1188,     0,  3959,   112,     2],\n",
       "        [   73,     0,   117,   391,   380,   363,    12,     9,  7859,    48],\n",
       "        [  394,     0,  2815,    45,    14,   547,   212,  1197,  1555,    63],\n",
       "        [   34,    25,   969,   487,  4073,    35,    62,   351,  1827,     3],\n",
       "        [ 2134,  2471,     6,     0,    21,  2130,   608,     3,  2786,  4332],\n",
       "        [    2,    71,     2,    57,     4,    18,  2481,   115,  3660,   489]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no separate labels. But you can just use an offset `batch.text[1:]` to get the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "Using the data given by this iterator, you should construct 3 different torch models that take in batch.text and produce a distribution over the next word. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw2-s18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final Kaggle test, we will have you do a next word prediction task. We will provide a 10 word prefix of sentences, and it is your job to predict 10 possible next word candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but while the new york stock exchange did n't fall ___\n",
      "some circuit breakers installed after the october N crash failed ___\n",
      "the N stock specialist firms on the big board floor ___\n",
      "big investment banks refused to step up to the plate ___\n",
      "heavy selling of standard & poor 's 500-stock index futures ___\n",
      "seven big board stocks ual amr bankamerica walt disney capital ___\n",
      "once again the specialists were not able to handle the ___\n",
      "<unk> james <unk> chairman of specialists henderson brothers inc. it ___\n",
      "when the dollar is in a <unk> even central banks ___\n",
      "speculators are calling for a degree of liquidity that is ___\n"
     ]
    }
   ],
   "source": [
    "!head input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sample Kaggle submission, let us build a simple unigram model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "count = Counter()\n",
    "for b in iter(train_iter):\n",
    "    count.update(b.text.view(-1).data.tolist())\n",
    "count[TEXT.vocab.stoi[\"<eos>\"]] = 0\n",
    "predictions = [TEXT.vocab.itos[i] for i, c in count.most_common(20)]\n",
    "with open(\"sample.txt\", \"w\") as fout: \n",
    "    print(\"id,word\", file=fout)\n",
    "    for i, l in enumerate(open(\"input.txt\"), 1):\n",
    "        print(\"%d,%s\"%(i, \" \".join(predictions)), file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,word\n",
      "1,the <unk> N of to a in and 's that for $ is it said on by at as from\n",
      "2,the <unk> N of to a in and 's that for $ is it said on by at as from\n",
      "3,the <unk> N of to a in and 's that for $ is it said on by at as from\n",
      "4,the <unk> N of to a in and 's that for $ is it said on by at as from\n",
      "5,the <unk> N of to a in and 's that for $ is it said on by at as from\n",
      "6,the <unk> N of to a in and 's that for $ is it said on by at as from\n",
      "7,the <unk> N of to a in and 's that for $ is it said on by at as from\n",
      "8,the <unk> N of to a in and 's that for $ is it said on by at as from\n",
      "9,the <unk> N of to a in and 's that for $ is it said on by at as from\n"
     ]
    }
   ],
   "source": [
    "!head sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric we are using is mean average precision of your 20-best list. \n",
    "\n",
    "$$MAP@20 = \\frac{1}{|D|} \\sum_{u=1}^{|D|} \\sum_{k=1}^{20} Precision(u, 1:k)$$\n",
    "\n",
    "Ideally we would use log-likelihood or ppl as discussed in class, but this is the best Kaggle gives us. This takes into account whether you got the right answer and how highly you ranked it. \n",
    "\n",
    "In particular, we ask that you do not game this metric. Please submit *exactly 20* unique predictions for each example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always you should put up a 5-6 page write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf61737332034684a9f63dbef2fbca8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2905), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ac6f0593b915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_tri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_tri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_tri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_tri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_text' is not defined"
     ]
    }
   ],
   "source": [
    "# Trigram model\n",
    "%run models/trigram.py\n",
    "%run models/utils.py\n",
    "\n",
    "model_tri = Trigram(TEXT)\n",
    "model_tri.get_probabilities(train_iter)\n",
    "out = model_tri.predict(batch_text)\n",
    "\n",
    "optimizer = torch.optim.Adam([model_tri.log_weights], lr=0.1)\n",
    "\n",
    "def cb(**kwargs):\n",
    "    print(kwargs['epoch'], kwargs['loss'].item(), \n",
    "          torch.softmax(model_tri.log_weights, dim=0))\n",
    "\n",
    "train_model(model_tri, trigram_loss_fn, optimizer, val_iter, val_iter=None,\n",
    "            inner_callback=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NNLang(nn.Sequential):\n",
    "#     def __init__(self, embedding_dim, hidden_dims, TEXT):\n",
    "#         super().__init__()\n",
    "#         self.embeds = nn.Embedding(len(TEXT.vocab), embedding_dim)\n",
    "#         self.self.Linear(5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models/neural_net_lang.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_nn_lang_model(embedding_dim=10,\n",
    "                        hidden=10,\n",
    "                        TEXT=TEXT,\n",
    "                        n_hidden_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Pad()\n",
       "  (1): Embedding(10001, 10)\n",
       "  (2): Flatten()\n",
       "  (3): Linear(in_features=320, out_features=10, bias=True)\n",
       "  (4): Tanh()\n",
       "  (5): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (6): Tanh()\n",
       "  (7): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (8): Tanh()\n",
       "  (9): Linear(in_features=10, out_features=10001, bias=True)\n",
       "  (10): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "hidden = 100\n",
    "seqlen = 32\n",
    "pad_i = TEXT.vocab.stoi['<pad>']\n",
    "\n",
    "class Pad(nn.Module):\n",
    "    def __init__(self, seqlen, pad_i):\n",
    "        super().__init__()\n",
    "        self.seqlen = seqlen\n",
    "        self.pad_i = pad_i\n",
    "    def forward(self, x):\n",
    "        init = torch.ones(self.seqlen, x.shape[1]) * pad_i\n",
    "        init[-x.shape[0]:, :] = x\n",
    "        return init.long()\n",
    "\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x.permute(1,0,2).flatten(start_dim=1,end_dim=2)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    Pad(seqlen, pad_i),\n",
    "    nn.Embedding(len(TEXT.vocab), embedding_dim),\n",
    "    Flatten(),\n",
    "    nn.Linear(seqlen * embedding_dim, hidden),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(hidden, len(TEXT.vocab)),\n",
    "    nn.Softmax(dim=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def loss_fn(model, batch):\n",
    "    return criterion(model(batch.text), batch.target[-1,:])\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4305e4f53b0741bfbfdca65f285f0bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2905), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26634.14468574524\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61d4e0166414a96b4c5fbaa96a0978e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2905), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26134.826528549194\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3b1268c0eb4267be798d931f959c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2905), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 25545.120619773865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c7484d6f1e43c1968b8d6d962fd538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2905), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 25191.55255126953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f841cea9ade14f3db59059e78d869b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2905), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 25005.816497802734\n"
     ]
    }
   ],
   "source": [
    "def cb(**kwargs):\n",
    "    print(kwargs['epoch'], kwargs['train_loss'])\n",
    "train_model(net, loss_fn=loss_fn, optimizer=optimizer, train_iter=train_iter, \n",
    "            callback=cb, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = next(iter(val_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consumers may want to move their telephones a little closer to the tv set <eos> <unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play\n",
      "sales/problems/comes/lose/future/mixte/focus/bills/officer/deposit/before/schemes/open/generally/again/sam/methods/contractor/consequences/denver\n",
      "in\n",
      "\n",
      "\n",
      "\n",
      "said <eos> he said he would n't comment on the cftc plan until the exchange has seen the full proposal <eos> but at a meeting last week tom <unk> the board of\n",
      "china/america/both/only/immediately/the/an/pacific/make/hbo/not/ge/acquisitions/rules/everyone/ad/lockheed/decision/expect/rose\n",
      "trade\n",
      "\n",
      "\n",
      "\n",
      "analysis <eos> he found N still <unk> and N fairly valued <eos> nicholas parks a new york money manager expects the market to decline about N N <eos> i 've been two-thirds\n",
      "know/include/groups/ministry/kageyama/half/galileo/profitable/made/offer/cities/fournier/sit/dead/division/compared/means/companies/decline/individual\n",
      "in\n",
      "\n",
      "\n",
      "\n",
      "novel of <unk> like <unk> herself still ruled the waves <eos> in fact <unk> <unk> 's the remains of the day <unk> N pages $ N is both an <unk> to traditional\n",
      "out/used/example/sat/traditionally/around/students/huge/wish/temporarily/homes/part/factors/charles/stronger/cooperation/bankruptcy/institutes/evident/supplies\n",
      "english\n",
      "\n",
      "\n",
      "\n",
      "makes some executives nervous <eos> last year the research and development division of weyerhaeuser co. the large <unk> concern invited a <unk> to its <unk> wash. offices <eos> phil <unk> a software\n",
      "u.s./latest/first/same/spokesman/outcome/japanese/family/sale/new/nation/case/market/york/fact/working/transaction/bank/third/meeting\n",
      "engineer\n",
      "\n",
      "\n",
      "\n",
      "more expensive than direct treasury borrowing said rep. <unk> stark d. calif. the bill 's chief sponsor <eos> the complex financing plan in the s&l bailout law includes raising $ N billion\n",
      "from/in/later/gary/president/<eos>/plants/people/or/active/hutton/rep./can/expires/such/distance/write-off/against/junk/wcrs\n",
      "from\n",
      "\n",
      "\n",
      "\n",
      "arizona market <eos> security pacific reported a N N increase in net credit losses for the quarter to $ N million from $ N million in the year-ago period <eos> nonperforming loans\n",
      "big/grew/side/industry/also/attorney/desert/operations/is/highly/probably/noriega/longer/amounts/vegas/&/results/reports/broke/widely\n",
      "grew\n",
      "\n",
      "\n",
      "\n",
      "york <eos> other asian and pacific markets had sharper losses than tokyo but the selling wave stopped short of <unk> another market crash <eos> all eyes were on tokyo at the opening\n",
      "are/billion/with/growth/run/according/countries/suggest/officials/conference/calls/why/'s/deals/university/jones/will/n't/states/high\n",
      "because\n",
      "\n",
      "\n",
      "\n",
      "<eos> traders said retail investors seemed to be <unk> the sidelines until a measure of volatility is <unk> out of the market <eos> new jersey turnpike authority 's N N issue of\n",
      "N/share/net/operation/assets/orders/analysts/banks/friday/london/deficit/nrm/financial/all/yield/modest/overnight/oct./index/notes\n",
      "N\n",
      "\n",
      "\n",
      "\n",
      "prices to rise with or without the stock market <eos> what the stock market did was cause the rise to take place earlier than it would have happened said mr. <unk> <eos>\n",
      "my/time/we/there/it/land/ibm/william/priority/lot/might/growing/readers/still/bonn/portfolio/firms/his/approximately/for\n",
      "there\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, arg = torch.topk( net(val_batch.text), 20)\n",
    "answers = [tensor_to_text(argmax, TEXT).split(' ') for argmax in arg]\n",
    "for i, ans in enumerate(answers):\n",
    "    print(f\"{tensor_to_text(val_batch.text[:, i], TEXT)}\\n{'/'.join(ans)}\\n{tensor_to_text(val_batch.target[-1, i].unsqueeze(0), TEXT)}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = nn.Embedding(len(TEXT.vocab), embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 640])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds(batch.text).permute(1,0,2).flatten(start_dim=1,end_dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Sets the module in training mode.\n",
       "\n",
       "This has any effect only on certain modules. See documentations of\n",
       "particular modules for details of their behaviors in training/evaluation\n",
       "mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
       "etc.\n",
       "\n",
       "Returns:\n",
       "    Module: self\n",
       "\u001b[0;31mFile:\u001b[0m      /anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
